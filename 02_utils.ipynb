{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "> The utility functions here can be used for training and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "log = logging.getLogger(\"Utilities for training classification models on the BreaKHis dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mixup_data(x, y, criterion, alpha=1.0):\n",
    "    \"\"\"Compute the mixup data for batch `x, y`. Return mixed inputs, pairs of targets, and lambda.\"\"\"\n",
    "    batch_size = x.size()[0]\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha, batch_size)\n",
    "        lam = np.concatenate(\n",
    "            [lam[:, None], 1 - lam[:, None]], 1\n",
    "        ).max(1)[:, None, None, None]\n",
    "        lam = torch.from_numpy(lam).float()\n",
    "        if torch.cuda.is_available():\n",
    "            lam = lam.cuda()\n",
    "    else:\n",
    "        lam = 1.\n",
    "    index = torch.randperm(batch_size)\n",
    "    if torch.cuda.is_available():\n",
    "        index = index.cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    mixed_y = (lam * y_a) + ((1 - lam) * y_b)\n",
    "    \n",
    "    def mixup_criterion(pred):\n",
    "        return (lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)).mean()\n",
    "    def mixup_acc(pred):\n",
    "        return (pred == mixed_y).sum().item()\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam, mixup_criterion, mixup_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is called in train if `mixup` is specified as true.\n",
    "* `x`, `y` should be `torch.Tensor`\n",
    "* `criterion` should be a `torch` loss function, e.g. `nn.CrossEntropyLoss`\n",
    "* `alpha` is a float defining the distribution for sampling the mixing value (see the Mixup paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(\n",
    "    model, epoch, dataloader, criterion, optimizer, scheduler=None, mixup=False, alpha=0.4,\n",
    "    logging_frequency=50\n",
    "):\n",
    "    \"\"\" Trains `model` on data in `dataloader` with loss `criterion` and optimization scheme\n",
    "        defined by `optimizer`, with optional learning schedule defined by `scheduler`. This\n",
    "        function calls performs 1 epoch - passing in `epoch` is purely for logging clarity.\n",
    "        Logs every `logging_frequency` iterations.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total, total_loss, total_correct = 0, 0., 0.\n",
    "    \n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        mixed_x, y_a, y_b, lam, mixup_criterion, mixup_acc = mixup_data(\n",
    "            x, y, criterion, alpha=alpha if mixup else 0.0\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mixed_x)\n",
    "        prediction = torch.argmax(output, -1)\n",
    "        loss = mixup_criterion(output)\n",
    "        total_loss += loss.item() * len(y)\n",
    "        total_correct += mixup_acc(prediction)\n",
    "        total += len(y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if i % logging_frequency == 0 and i > 0:\n",
    "            \"\"\" TODO:\n",
    "            Add Tensorboard functionality here - mainly writer.add_scalar for\n",
    "            overall loss, accuracy (i.e. over all epochs).\n",
    "            \"\"\"\n",
    "            log.error(\n",
    "                \"[Epoch %d, Iteration %d / %d] Training Loss: %.5f, \"\n",
    "                \"Training Accuracy: %.5f [Projected Accuracy: %.5f]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    total_loss / total,\n",
    "                    total_correct / len(dataloader.dataset),\n",
    "                    (total_correct / len(dataloader.dataset)) / (i / len(dataloader))\n",
    "                )\n",
    "            )\n",
    "    final_loss, final_acc = total_loss / total, total_correct / total\n",
    "    log.info(\n",
    "        \"Reporting %.5f training loss, %.5f training accuracy for epoch %d.\" % \n",
    "        (final_loss, final_acc, epoch)\n",
    "    )\n",
    "    return final_loss, final_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs 1 epoch of training.\n",
    "* `model` should be a `torch.nn.Module`\n",
    "* `epoch` should indicate the current epoch of training, and is only really necessary for logging purposes.\n",
    "* `dataloader should be a `torch.utils.data.DataLoader` wrapping a `BreaKHisDataset` object\n",
    "* `criterion` should be a `torch` loss function\n",
    "* `optimizer` should be a `torch.optim.Optimizer`, e.g. Adam\n",
    "* `scheduler` is optional, but when included, should be a `torch.optim._LRScheduler`, e.g. CyclicLR\n",
    "* `mixup` is a boolean indicating whether to use mixup augmentation for training (default is False)\n",
    "* `alpha` is a float determining the distribution for sampling the mixing ratio\n",
    "* `logging_frequency` determines the cycle of iterations before logging metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate(model, epoch, dataloader, criterion, tta=False, tta_mixing=0.6, logging_frequency=50):\n",
    "    \"\"\"Validates `model` on data in `dataloader` for epoch `epoch` using objective `criterion`.\"\"\"\n",
    "    model.eval()\n",
    "    total, total_loss, total_correct = 0, 0., 0.\n",
    "\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        with torch.no_grad():\n",
    "            if tta:\n",
    "                bs, n_aug, c, h, w = x.size()\n",
    "                output = model(x.view(-1, c, h, w)).view(bs, n_aug, -1)\n",
    "                output = (\n",
    "                    ((1 - tta_mixing) * output[:, -1, :]) + (tta_mixing * output[:, :-1, :].mean(1))\n",
    "                )\n",
    "            else:\n",
    "                output = model(x)\n",
    "            prediction = torch.argmax(output, -1)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item() * len(y)\n",
    "            total_correct += (prediction == y).sum().item()\n",
    "            total += len(y)\n",
    "\n",
    "        if i % logging_frequency == 0 and i > 0:\n",
    "            \"\"\" TODO:\n",
    "            Add Tensorboard functionality here - mainly writer.add_scalar for\n",
    "            overall loss, accuracy (i.e. over all epochs).\n",
    "            \"\"\"\n",
    "            log.error(\n",
    "                \"[Epoch %d, Iteration %d / %d] Validation Loss: %.5f, \"\n",
    "                \"Validation Accuracy: %.5f [Validation Accuracy: %.5f]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    total_loss / total,\n",
    "                    total_correct / len(dataloader.dataset),\n",
    "                    (total_correct / len(dataloader.dataset)) / (i / len(dataloader))\n",
    "                )\n",
    "            )\n",
    "    final_loss, final_acc = total_loss / total, total_correct / total\n",
    "    log.info(\n",
    "        \"Reporting %.5f validation loss, %.5f validation accuracy for epoch %d.\" % \n",
    "        (final_loss, final_acc, epoch)\n",
    "    )\n",
    "    return final_loss, final_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs 1 epoch of validation.\n",
    "* `model` should be a `torch.nn.Module`\n",
    "* `epoch` should indicate the current epoch of training, and is only really necessary for logging purposes.\n",
    "* `dataloader should be a `torch.utils.data.DataLoader` wrapping a `BreaKHisDataset` object\n",
    "* `criterion` should be a `torch` loss function\n",
    "* `optimizer` should be a `torch.optim.Optimizer`, e.g. Adam\n",
    "* `tta` is a boolean indicating whether to use test-time augmentation (default is False)\n",
    "* `tta_mixing` determines how much of the test-time augmented data to use in determining the final output (default is 0.6)\n",
    "* `logging_frequency` determines the cycle of iterations before logging metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some toy examples using the functions defined above. For brevity, we use a small subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from breakhis_gradcam.data import initialize_datasets\n",
    "from breakhis_gradcam.resnet import resnet18\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_tta_transforms(resize_shape, normalize_transform, n=5):\n",
    "    tta = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomResizedCrop((resize_shape, resize_shape)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    original_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_shape, resize_shape)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(\n",
    "            lambda image: torch.stack(\n",
    "                [tta(image) for _ in range(n)] + [original_transform(image)]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda images: torch.stack([\n",
    "                normalize_transform(image) for image in images\n",
    "            ])\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "def get_transforms(resize_shape, tta=False, tta_n=5):\n",
    "    random_resized_crop = transforms.RandomResizedCrop((resize_shape, resize_shape))\n",
    "    random_horizontal_flip = transforms.RandomHorizontalFlip()\n",
    "    resize = transforms.Resize((resize_shape, resize_shape))\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transforms = transforms.Compose([\n",
    "        random_resized_crop, random_horizontal_flip, transforms.ToTensor(), normalize\n",
    "    ])\n",
    "    val_transforms = (\n",
    "        get_tta_transforms(resize_shape, normalize, n=tta_n) if tta\n",
    "        else transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
    "    )\n",
    "    return train_transforms, val_transforms\n",
    "    \n",
    "train_transform, val_transform = get_transforms(224, tta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mapping = initialize_datasets(\n",
    "    '/share/nikola/export/dt372/BreaKHis_v1/',\n",
    "    label='tumor_class', criterion=['tumor_type', 'magnification'],\n",
    "    split_transforms={'train': train_transform, 'val': val_transform}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds, val_ds = ds_mapping['train'], ds_mapping['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 0, Iteration 25 / 198] Training Loss: 0.46333, Training Accuracy: 2.17543 [Projected Accuracy: 17.22939]\n",
      "[Epoch 0, Iteration 50 / 198] Training Loss: 0.47674, Training Accuracy: 4.24937 [Projected Accuracy: 16.82749]\n",
      "[Epoch 0, Iteration 75 / 198] Training Loss: 0.48411, Training Accuracy: 6.35830 [Projected Accuracy: 16.78590]\n",
      "[Epoch 0, Iteration 100 / 198] Training Loss: 0.47731, Training Accuracy: 8.50142 [Projected Accuracy: 16.83282]\n",
      "[Epoch 0, Iteration 125 / 198] Training Loss: 0.47459, Training Accuracy: 10.64962 [Projected Accuracy: 16.86900]\n",
      "[Epoch 0, Iteration 150 / 198] Training Loss: 0.47411, Training Accuracy: 12.74208 [Projected Accuracy: 16.81955]\n",
      "[Epoch 0, Iteration 175 / 198] Training Loss: 0.47475, Training Accuracy: 14.79908 [Projected Accuracy: 16.74410]\n",
      "[Epoch 0, Iteration 25 / 50] Validation Loss: 0.42023, Validation Accuracy: 0.44005 [Validation Accuracy: 0.88010]\n"
     ]
    }
   ],
   "source": [
    "tr_loss, tr_acc = train(\n",
    "    model, 0, tr_dl, criterion['train'], optimizer, mixup=mixup, alpha=0.4,\n",
    "    logging_frequency=25\n",
    ")\n",
    "val_loss, val_acc = validate(\n",
    "    model, 0, val_dl, criterion['val'], tta=True,\n",
    "    logging_frequency=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_param_lr_maps(model, base_lr, finetune_body_factor):\n",
    "    \"\"\" Output parameter LR mappings for setting up an optimizer for `model`.\"\"\"\n",
    "    body_parameters = [\n",
    "        (param, _) for (param, _) in model.named_parameters() if param.split('.')[0] != 'out_fc'\n",
    "    ]\n",
    "    if type(finetune_body_factor) is float:\n",
    "        log.error(\n",
    "            \"Setting up optimizer to fine-tune body with LR %.8f and head with LR %.5f\" %\n",
    "            (base_lr * finetune_body_factor, base_lr)\n",
    "        )\n",
    "        return [\n",
    "            {'params': body_parameters, 'lr': base_lr * finetune_body_factor},\n",
    "            {'params': model.out_fc.parameters(), 'lr': base_lr}\n",
    "        ]\n",
    "    else:\n",
    "        lower_bound_factor, upper_bound_factor = finetune_body_factor\n",
    "        log.error(\n",
    "            \"Setting up optimizer to fine-tune body with LR in range [%.8f, %.8f]\"\n",
    "            \" and head with LR %.5f\" %\n",
    "            (base_lr * lower_bound_factor, base_lr * upper_bound_factor, base_lr)\n",
    "        )\n",
    "        lrs = np.geomspace(\n",
    "            base_lr * lower_bound_factor, base_lr * upper_bound_factor,\n",
    "            len(body_parameters)\n",
    "        )\n",
    "        param_lr_maps = [\n",
    "            {'params': param, 'lr': lr} for ((_, param), lr) in\n",
    "            zip(body_parameters, lrs)\n",
    "        ]\n",
    "        param_lr_maps.append({'params': model.out_fc.parameters(), 'lr': base_lr})\n",
    "        return param_lr_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is useful for setting up parameter to LR mappings for fine-tuning the model. Specifically:\n",
    "* `model` should be a `torch.nn.Module`\n",
    "* `base_lr` should be a float, defining the LR for the linear head\n",
    "* `finetune_body_factor` should be a list of two floats: a lower bound factor and upper bound factor. The learning rate for the body of the model will be equally (log) spaced between (`base_lr` * `lower_bound_factor`) and (`base_lr` * `upper_bound_factor`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_optimizer_and_scheduler(param_lr_maps, base_lr, epochs, steps_per_epoch):\n",
    "    \"\"\"Create a PyTorch AdamW optimizer and OneCycleLR scheduler with `param_lr_maps` parameter mapping,\n",
    "       with base LR `base_lr`, for training for `epochs` epochs, with `steps_per_epoch` iterations\n",
    "       per epoch.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(param_lr_maps, lr=base_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, base_lr, epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def checkpoint_state(\n",
    "    model, epoch, optimizer, scheduler, train_loss, train_acc, val_loss, val_acc,\n",
    "    model_dir='/share/nikola/export/dt372/breakhis_gradcam/models'\n",
    "):\n",
    "    \"\"\"Checkpoint the state of the system, including `model` state, `optimizer` state, `scheduler`\n",
    "       state, for `epoch`, saving the metrics as well.\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    try:\n",
    "        getattr(model, 'save_dir')\n",
    "    except BaseException:\n",
    "        setattr(\n",
    "            model, 'save_dir',\n",
    "            os.path.join(model_dir, time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "        )\n",
    "        os.mkdir(model.save_dir)\n",
    "    torch.save(\n",
    "        {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': None if scheduler is None else scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'epoch': epoch\n",
    "        },\n",
    "        os.path.join(model.save_dir, 'epoch_%d.pth' % epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, you can see how to set up the optimizer and scheduler to fine-tune using the one-cycle LR scheme. The linear head is fine-tuned with a learning rate of $10^{-3}$, and the body is fine-tuned with a learning rate spaced between $10^{-8}$ and $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up optimizer to fine-tune body with LR in range [0.00000001, 0.00001000] and head with LR 0.00100\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(pretrained=True, num_classes=2)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "mixup = True\n",
    "num_epochs = 5\n",
    "base_lr = 1e-3\n",
    "finetune_body_factor = [1e-5, 1e-2]\n",
    "param_lr_maps = get_param_lr_maps(model, base_lr, finetune_body_factor)\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(param_lr_maps, base_lr, num_epochs, len(tr_dl))\n",
    "criterion = {\n",
    "    'train': nn.CrossEntropyLoss(reduction='none' if mixup else 'mean'),\n",
    "    'val': nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple training loop would look like the following. Note that:\n",
    "* The one-cycle LR scheduler is passed in, and the logic for updating that is handled in `train`\n",
    "* Different criterion are used for training and validation. This is because the criterion for mixup is different for each batch, due to the mixing factor, so the criterion is modified in the loop for training, so the reduction is handled there, whereas reduction is standard when evaluating in validation (i.e. mean reduction)\n",
    "* Test-time augmentation is done in validation. Note that this will require having a special augmentation scheme, so validation transforms will need to be set appropriately. You can see above for an example of how to do that.\n",
    "* The model state is checkpointed each epoch. After checkpointing the state of the model and system, the directory where the state was saved can be accessed by inspecting `model.save_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Iteration 25 / 198] Training Loss: 0.61779, Training Accuracy: 1.66688 [Projected Accuracy: 13.20167]\n",
      "[Epoch 1, Iteration 50 / 198] Training Loss: 0.53964, Training Accuracy: 3.76124 [Projected Accuracy: 14.89452]\n",
      "[Epoch 1, Iteration 75 / 198] Training Loss: 0.50664, Training Accuracy: 5.83344 [Projected Accuracy: 15.40028]\n",
      "[Epoch 1, Iteration 100 / 198] Training Loss: 0.50104, Training Accuracy: 7.81111 [Projected Accuracy: 15.46601]\n",
      "[Epoch 1, Iteration 125 / 198] Training Loss: 0.49492, Training Accuracy: 10.04433 [Projected Accuracy: 15.91022]\n",
      "[Epoch 1, Iteration 150 / 198] Training Loss: 0.48815, Training Accuracy: 12.14424 [Projected Accuracy: 16.03039]\n",
      "[Epoch 1, Iteration 175 / 198] Training Loss: 0.48081, Training Accuracy: 14.39566 [Projected Accuracy: 16.28766]\n",
      "[Epoch 1, Iteration 25 / 50] Validation Loss: 0.50810, Validation Accuracy: 0.38920 [Validation Accuracy: 0.77841]\n",
      "[Epoch 2, Iteration 25 / 198] Training Loss: 0.44745, Training Accuracy: 2.18651 [Projected Accuracy: 17.31716]\n",
      "[Epoch 2, Iteration 50 / 198] Training Loss: 0.44490, Training Accuracy: 4.31444 [Projected Accuracy: 17.08518]\n",
      "[Epoch 2, Iteration 75 / 198] Training Loss: 0.43113, Training Accuracy: 6.45250 [Projected Accuracy: 17.03460]\n",
      "[Epoch 2, Iteration 100 / 198] Training Loss: 0.42757, Training Accuracy: 8.56523 [Projected Accuracy: 16.95916]\n",
      "[Epoch 2, Iteration 125 / 198] Training Loss: 0.42541, Training Accuracy: 10.73876 [Projected Accuracy: 17.01019]\n",
      "[Epoch 2, Iteration 150 / 198] Training Loss: 0.42320, Training Accuracy: 12.91450 [Projected Accuracy: 17.04714]\n",
      "[Epoch 2, Iteration 175 / 198] Training Loss: 0.42172, Training Accuracy: 15.05478 [Projected Accuracy: 17.03341]\n",
      "[Epoch 2, Iteration 25 / 50] Validation Loss: 0.31465, Validation Accuracy: 0.47646 [Validation Accuracy: 0.95292]\n",
      "[Epoch 3, Iteration 25 / 198] Training Loss: 0.41465, Training Accuracy: 2.23876 [Projected Accuracy: 17.73097]\n",
      "[Epoch 3, Iteration 50 / 198] Training Loss: 0.39440, Training Accuracy: 4.43762 [Projected Accuracy: 17.57297]\n",
      "[Epoch 3, Iteration 75 / 198] Training Loss: 0.40363, Training Accuracy: 6.61241 [Projected Accuracy: 17.45677]\n",
      "[Epoch 3, Iteration 100 / 198] Training Loss: 0.40729, Training Accuracy: 8.78230 [Projected Accuracy: 17.38895]\n",
      "[Epoch 3, Iteration 125 / 198] Training Loss: 0.40448, Training Accuracy: 11.01662 [Projected Accuracy: 17.45033]\n",
      "[Epoch 3, Iteration 150 / 198] Training Loss: 0.40535, Training Accuracy: 13.25095 [Projected Accuracy: 17.49125]\n",
      "[Epoch 3, Iteration 175 / 198] Training Loss: 0.40613, Training Accuracy: 15.48638 [Projected Accuracy: 17.52174]\n",
      "[Epoch 3, Iteration 25 / 50] Validation Loss: 0.55899, Validation Accuracy: 0.35782 [Validation Accuracy: 0.71563]\n",
      "[Epoch 4, Iteration 25 / 198] Training Loss: 0.39973, Training Accuracy: 2.23939 [Projected Accuracy: 17.73598]\n",
      "[Epoch 4, Iteration 50 / 198] Training Loss: 0.39833, Training Accuracy: 4.40279 [Projected Accuracy: 17.43503]\n",
      "[Epoch 4, Iteration 75 / 198] Training Loss: 0.38543, Training Accuracy: 6.70108 [Projected Accuracy: 17.69084]\n",
      "[Epoch 4, Iteration 100 / 198] Training Loss: 0.38818, Training Accuracy: 8.88474 [Projected Accuracy: 17.59178]\n",
      "[Epoch 4, Iteration 125 / 198] Training Loss: 0.38387, Training Accuracy: 11.10038 [Projected Accuracy: 17.58300]\n",
      "[Epoch 4, Iteration 150 / 198] Training Loss: 0.37873, Training Accuracy: 13.32695 [Projected Accuracy: 17.59157]\n",
      "[Epoch 4, Iteration 175 / 198] Training Loss: 0.37771, Training Accuracy: 15.57853 [Projected Accuracy: 17.62599]\n",
      "[Epoch 4, Iteration 25 / 50] Validation Loss: 0.29945, Validation Accuracy: 0.48964 [Validation Accuracy: 0.97928]\n",
      "[Epoch 5, Iteration 25 / 198] Training Loss: 0.35548, Training Accuracy: 2.35893 [Projected Accuracy: 18.68272]\n",
      "[Epoch 5, Iteration 50 / 198] Training Loss: 0.34854, Training Accuracy: 4.55272 [Projected Accuracy: 18.02878]\n",
      "[Epoch 5, Iteration 75 / 198] Training Loss: 0.35551, Training Accuracy: 6.67052 [Projected Accuracy: 17.61017]\n",
      "[Epoch 5, Iteration 100 / 198] Training Loss: 0.35722, Training Accuracy: 8.90991 [Projected Accuracy: 17.64162]\n",
      "[Epoch 5, Iteration 125 / 198] Training Loss: 0.35682, Training Accuracy: 11.05351 [Projected Accuracy: 17.50877]\n",
      "[Epoch 5, Iteration 150 / 198] Training Loss: 0.35697, Training Accuracy: 13.18857 [Projected Accuracy: 17.40891]\n",
      "[Epoch 5, Iteration 175 / 198] Training Loss: 0.35440, Training Accuracy: 15.39756 [Projected Accuracy: 17.42124]\n",
      "[Epoch 5, Iteration 25 / 50] Validation Loss: 0.31663, Validation Accuracy: 0.48023 [Validation Accuracy: 0.96045]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    tr_loss, tr_acc = train(\n",
    "        model, epoch + 1, tr_dl, criterion['train'], optimizer, scheduler=scheduler,\n",
    "        mixup=mixup, alpha=0.4, logging_frequency=25\n",
    "    )\n",
    "    val_loss, val_acc = validate(\n",
    "        model, epoch + 1, val_dl, criterion['val'], tta=True,\n",
    "        logging_frequency=25\n",
    "    )\n",
    "    checkpoint_state(\n",
    "        model, epoch + 1, optimizer, scheduler, tr_loss, tr_acc, val_loss, val_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch_1.pth', 'epoch_2.pth', 'epoch_3.pth', 'epoch_4.pth', 'epoch_5.pth']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(model.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just use the validate method with some slight alterations to get the standard training accuracy (not the mixup accracy, which might not be as representative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5, Iteration 25 / 198] Validation Loss: 0.16696, Validation Accuracy: 0.12666 [Validation Accuracy: 1.00317]\n",
      "[Epoch 5, Iteration 50 / 198] Validation Loss: 0.17132, Validation Accuracy: 0.24731 [Validation Accuracy: 0.97934]\n",
      "[Epoch 5, Iteration 75 / 198] Validation Loss: 0.17119, Validation Accuracy: 0.36859 [Validation Accuracy: 0.97307]\n",
      "[Epoch 5, Iteration 100 / 198] Validation Loss: 0.16980, Validation Accuracy: 0.49003 [Validation Accuracy: 0.97025]\n",
      "[Epoch 5, Iteration 125 / 198] Validation Loss: 0.16848, Validation Accuracy: 0.61257 [Validation Accuracy: 0.97031]\n",
      "[Epoch 5, Iteration 150 / 198] Validation Loss: 0.16889, Validation Accuracy: 0.73385 [Validation Accuracy: 0.96868]\n",
      "[Epoch 5, Iteration 175 / 198] Validation Loss: 0.16793, Validation Accuracy: 0.85529 [Validation Accuracy: 0.96770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy after 5 epochs is 0.95836\n"
     ]
    }
   ],
   "source": [
    "_, tr_acc_no_mixup = validate(model, epoch + 1, tr_dl, criterion['val'], tta=False, logging_frequency=25)\n",
    "print(\"Training accuracy after %d epochs is %.5f\" % (epoch + 1, tr_acc_no_mixup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
