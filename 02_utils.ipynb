{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "> The utility functions here can be used for training and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "log = logging.getLogger('Metrics')\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mixup_data(x, y, criterion, alpha=1.0):\n",
    "    \"\"\"Compute the mixup data for batch `x, y`. Return mixed inputs, pairs of targets, and lambda.\"\"\"\n",
    "    batch_size = x.size()[0]\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha, batch_size)\n",
    "        lam = np.concatenate(\n",
    "            [lam[:, None], 1 - lam[:, None]], 1\n",
    "        ).max(1)[:, None, None, None]\n",
    "        lam = torch.from_numpy(lam).float()\n",
    "        if torch.cuda.is_available():\n",
    "            lam = lam.cuda()\n",
    "    else:\n",
    "        lam = 1.\n",
    "    index = torch.randperm(batch_size)\n",
    "    if torch.cuda.is_available():\n",
    "        index = index.cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    mixed_y = (lam * y_a) + ((1 - lam) * y_b)\n",
    "    \n",
    "    def mixup_criterion(pred):\n",
    "        return (lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)).mean()\n",
    "    def mixup_acc(pred):\n",
    "        return (pred == mixed_y).sum().item()\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam, mixup_criterion, mixup_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is called in train if `mixup` is specified as true.\n",
    "* `x`, `y` should be `torch.Tensor`\n",
    "* `criterion` should be a `torch` loss function, e.g. `nn.CrossEntropyLoss`\n",
    "* `alpha` is a float defining the distribution for sampling the mixing value (see the Mixup paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_logging_streams(model, log_to_file=True, log_to_stdout=False):\n",
    "    \"\"\"Utility function for setting up logging handlers for `model`.\"\"\"\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(name)s][%(asctime)s][%(levelname)s]: %(message)s',\n",
    "        datefmt='%m:%d:%Y:%I:%M:%S'\n",
    "    )\n",
    "    handlers = []\n",
    "    if log_to_file:\n",
    "        fh = logging.FileHandler(os.path.join(model.log_dir, 'metrics.log'))\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(formatter)\n",
    "        log.addHandler(fh)\n",
    "        print(\"Logging to %s\" % os.path.join(model.log_dir, 'metrics.log'))\n",
    "        handlers.append(fh)\n",
    "\n",
    "    if log_to_stdout:\n",
    "        ch = logging.StreamHandler(stream=sys.stdout)\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        log.addHandler(ch)\n",
    "        print(\"Logging to STDOUT\")\n",
    "        handlers.append(ch)\n",
    "\n",
    "    def clear_handlers():\n",
    "        for handler in handlers:\n",
    "            handler.close()\n",
    "            log.removeHandler(handler)\n",
    "        print(\"Cleared all logging handlers\")\n",
    "\n",
    "    return clear_handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps set up logging functionality for readable outputted metrics.\n",
    "* `model` should be the model constructed from the ResNet helper functions. They are initialized with logging and output directories by default, as long as you specify existing overarching model and logging directories.\n",
    "* `log_to_file` specifies whether to output to a metrics log file in the model's logging directory\n",
    "* `log_to_stdout` specifies whether to output metrics to STDOUT\n",
    "\n",
    "The function returns a closure that, when called, will clear any handlers set up in the logging module for outputting to log file or STDOUT, depending on what was specified. To avoid any confusion when logging between training runs in the same notebook, it's important to call this closure to not have redundant logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(\n",
    "    model, epoch, dataloader, criterion, optimizer, scheduler=None, mixup=False, alpha=0.4,\n",
    "    logging_frequency=50\n",
    "):\n",
    "    \"\"\" Trains `model` on data in `dataloader` with loss `criterion` and optimization scheme\n",
    "        defined by `optimizer`, with optional learning schedule defined by `scheduler`.\"\"\"\n",
    "    model.train()\n",
    "    total, total_loss, total_correct = 0, 0., 0.\n",
    "    \n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        mixed_x, y_a, y_b, lam, mixup_criterion, mixup_acc = mixup_data(\n",
    "            x, y, criterion, alpha=alpha if mixup else 0.0\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mixed_x)\n",
    "        prediction = torch.argmax(output, -1)\n",
    "        loss = mixup_criterion(output)\n",
    "        total_loss += loss.item() * len(y)\n",
    "        total_correct += mixup_acc(prediction)\n",
    "        total += len(y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if i % logging_frequency == 0 and i > 0:\n",
    "            \"\"\" TODO:\n",
    "            Add Tensorboard functionality here - mainly writer.add_scalar for\n",
    "            overall loss, accuracy (i.e. over all epochs).\n",
    "            \"\"\"\n",
    "            log.debug(\n",
    "                \"[Epoch %d, Iteration %d / %d] Training Loss: %.5f, \"\n",
    "                \"Training Accuracy: %.5f [Projected Accuracy: %.5f]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    total_loss / total,\n",
    "                    total_correct / len(dataloader.dataset),\n",
    "                    (total_correct / len(dataloader.dataset)) / (i / len(dataloader))\n",
    "                )\n",
    "            )\n",
    "    final_loss, final_acc = total_loss / total, total_correct / total\n",
    "    log.info(\n",
    "        \"Reporting %.5f training loss, %.5f training accuracy for epoch %d.\" % \n",
    "        (final_loss, final_acc, epoch)\n",
    "    )\n",
    "    return final_loss, final_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs 1 epoch of training.\n",
    "* `model` should be a `torch.nn.Module`\n",
    "* `epoch` should indicate the current epoch of training, and is only really necessary for logging purposes.\n",
    "* `dataloader should be a `torch.utils.data.DataLoader` wrapping a `BreaKHisDataset` object\n",
    "* `criterion` should be a `torch` loss function\n",
    "* `optimizer` should be a `torch.optim.Optimizer`, e.g. Adam\n",
    "* `scheduler` is optional, but when included, should be a `torch.optim._LRScheduler`, e.g. CyclicLR\n",
    "* `mixup` is a boolean indicating whether to use mixup augmentation for training (default is False)\n",
    "* `alpha` is a float determining the distribution for sampling the mixing ratio\n",
    "* `logging_frequency` determines the cycle of iterations before logging metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate(\n",
    "    model, epoch, dataloader, criterion, tta=False, tta_mixing=0.6, logging_frequency=50\n",
    "):\n",
    "    \"\"\"Validates `model` on data in `dataloader` for epoch `epoch` using objective `criterion`.\"\"\"\n",
    "    model.eval()\n",
    "    total, total_loss, total_correct = 0, 0., 0.\n",
    "\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        with torch.no_grad():\n",
    "            if tta:\n",
    "                bs, n_aug, c, h, w = x.size()\n",
    "                output = model(x.view(-1, c, h, w)).view(bs, n_aug, -1)\n",
    "                output = (\n",
    "                    ((1 - tta_mixing) * output[:, -1, :]) + (tta_mixing * output[:, :-1, :].mean(1))\n",
    "                )\n",
    "            else:\n",
    "                output = model(x)\n",
    "            prediction = torch.argmax(output, -1)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item() * len(y)\n",
    "            total_correct += (prediction == y).sum().item()\n",
    "            total += len(y)\n",
    "\n",
    "        if i % logging_frequency == 0 and i > 0:\n",
    "            \"\"\" TODO:\n",
    "            Add Tensorboard functionality here - mainly writer.add_scalar for\n",
    "            overall loss, accuracy (i.e. over all epochs).\n",
    "            \"\"\"\n",
    "            log.debug(\n",
    "                \"[Epoch %d, Iteration %d / %d] Validation Loss: %.5f, \"\n",
    "                \"Validation Accuracy: %.5f [Projected Accuracy: %.5f]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    total_loss / total,\n",
    "                    total_correct / len(dataloader.dataset),\n",
    "                    (total_correct / len(dataloader.dataset)) / (i / len(dataloader))\n",
    "                )\n",
    "            )\n",
    "    final_loss, final_acc = total_loss / total, total_correct / total\n",
    "    log.info(\n",
    "        \"Reporting %.5f validation loss, %.5f validation accuracy for epoch %d.\" % \n",
    "        (final_loss, final_acc, epoch)\n",
    "    )\n",
    "    return final_loss, final_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs 1 epoch of validation.\n",
    "* `model` should be a `torch.nn.Module`\n",
    "* `epoch` should indicate the current epoch of training, and is only really necessary for logging purposes.\n",
    "* `dataloader should be a `torch.utils.data.DataLoader` wrapping a `BreaKHisDataset` object\n",
    "* `criterion` should be a `torch` loss function\n",
    "* `optimizer` should be a `torch.optim.Optimizer`, e.g. Adam\n",
    "* `tta` is a boolean indicating whether to use test-time augmentation (default is False)\n",
    "* `tta_mixing` determines how much of the test-time augmented data to use in determining the final output (default is 0.6)\n",
    "* `logging_frequency` determines the cycle of iterations before logging metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some toy examples using the functions defined above. For brevity, we use a small subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "from breakhis_gradcam.data import initialize_datasets\n",
    "from breakhis_gradcam.resnet import resnet18\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_tta_transforms(resize_shape, normalize_transform, n=5):\n",
    "    tta = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomResizedCrop((resize_shape, resize_shape)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    original_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_shape, resize_shape)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(\n",
    "            lambda image: torch.stack(\n",
    "                [tta(image) for _ in range(n)] + [original_transform(image)]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda images: torch.stack([\n",
    "                normalize_transform(image) for image in images\n",
    "            ])\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "def get_transforms(resize_shape, tta=False, tta_n=5):\n",
    "    random_resized_crop = transforms.RandomResizedCrop((resize_shape, resize_shape))\n",
    "    random_horizontal_flip = transforms.RandomHorizontalFlip()\n",
    "    resize = transforms.Resize((resize_shape, resize_shape))\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transforms = transforms.Compose([\n",
    "        random_resized_crop, random_horizontal_flip, transforms.ToTensor(), normalize\n",
    "    ])\n",
    "    val_transforms = (\n",
    "        get_tta_transforms(resize_shape, normalize, n=tta_n) if tta\n",
    "        else transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
    "    )\n",
    "    return train_transforms, val_transforms\n",
    "    \n",
    "train_transform, val_transform = get_transforms(224, tta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "ds_mapping = initialize_datasets(\n",
    "    '/share/nikola/export/dt372/BreaKHis_v1/',\n",
    "    label='tumor_class', criterion=['tumor_type', 'magnification'],\n",
    "    split_transforms={'train': train_transform, 'val': val_transform}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "tr_ds, val_ds = ds_mapping['train'], ds_mapping['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "model = resnet18(pretrained=True, num_classes=2)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.AdamW([{'params': model.out_fc.parameters(), 'lr': 1e-3}])\n",
    "mixup = True\n",
    "criterion = {\n",
    "    'train': nn.CrossEntropyLoss(reduction='none' if mixup else 'mean'),\n",
    "    'val': nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop might include something like the following. Note the calls to `clear_logging_handlers` - this should be included in your code as well to avoid logging redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /share/nikola/export/dt372/breakhis_gradcam/logs/2020-02-16-23-18-10/metrics.log\n",
      "Logging to STDOUT\n",
      "[Metrics][02:16:2020:06:18:29][DEBUG]: [Epoch 0, Iteration 25 / 198] Training Loss: 0.63333, Training Accuracy: 1.94728 [Projected Accuracy: 15.42243]\n",
      "[Metrics][02:16:2020:06:18:41][DEBUG]: [Epoch 0, Iteration 50 / 198] Training Loss: 0.58616, Training Accuracy: 3.94854 [Projected Accuracy: 15.63623]\n",
      "[Metrics][02:16:2020:06:18:54][DEBUG]: [Epoch 0, Iteration 75 / 198] Training Loss: 0.56723, Training Accuracy: 5.90912 [Projected Accuracy: 15.60008]\n",
      "[Metrics][02:16:2020:06:19:07][DEBUG]: [Epoch 0, Iteration 100 / 198] Training Loss: 0.55472, Training Accuracy: 7.89645 [Projected Accuracy: 15.63498]\n",
      "[Metrics][02:16:2020:06:19:21][DEBUG]: [Epoch 0, Iteration 125 / 198] Training Loss: 0.54388, Training Accuracy: 9.99066 [Projected Accuracy: 15.82520]\n",
      "[Metrics][02:16:2020:06:19:34][DEBUG]: [Epoch 0, Iteration 150 / 198] Training Loss: 0.53556, Training Accuracy: 12.05890 [Projected Accuracy: 15.91775]\n",
      "[Metrics][02:16:2020:06:19:47][DEBUG]: [Epoch 0, Iteration 175 / 198] Training Loss: 0.52685, Training Accuracy: 14.26282 [Projected Accuracy: 16.13737]\n",
      "[Metrics][02:16:2020:06:19:58][INFO]: Reporting 0.52171 training loss, 15.97910 training accuracy for epoch 0.\n",
      "[Metrics][02:16:2020:06:20:50][DEBUG]: [Epoch 0, Iteration 25 / 50] Validation Loss: 0.41677, Validation Accuracy: 0.46014 [Validation Accuracy: 0.92028]\n",
      "[Metrics][02:16:2020:06:21:36][INFO]: Reporting 0.41893 validation loss, 0.87947 validation accuracy for epoch 0.\n",
      "Cleared all logging handlers\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "clear_logging_handlers = setup_logging_streams(model, log_to_file=True, log_to_stdout=True)\n",
    "try:\n",
    "    tr_loss, tr_acc = train(\n",
    "        model, 0, tr_dl, criterion['train'], optimizer, mixup=mixup, alpha=0.4,\n",
    "        logging_frequency=25\n",
    "    )\n",
    "    val_loss, val_acc = validate(\n",
    "        model, 0, val_dl, criterion['val'], tta=True,\n",
    "        logging_frequency=25\n",
    "    )\n",
    "except BaseException:\n",
    "    clear_logging_handlers()\n",
    "finally:\n",
    "    clear_logging_handlers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we were just testing here, it might be annoying to have find the log and state files later to remove for saving memory. However, we can just do the following to resolve that (this will delete all the contents to clear the log and model/system state directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing directory /share/nikola/export/dt372/breakhis_gradcam/logs/2020-02-16-23-18-10 and all contents.\n",
      "Removing directory /share/nikola/export/dt372/breakhis_gradcam/models/2020-02-16-23-18-10 and all contents.\n",
      "Resetting /share/nikola/export/dt372/breakhis_gradcam/logs/2020-02-16-23-18-10 and /share/nikola/export/dt372/breakhis_gradcam/models/2020-02-16-23-18-10.\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "model.clear_logging_and_output_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_param_lr_maps(model, base_lr, finetune_body_factor):\n",
    "    \"\"\" Output parameter LR mappings for setting up an optimizer for `model`.\"\"\"\n",
    "    body_parameters = [\n",
    "        (param, _) for (param, _) in model.named_parameters() if param.split('.')[0] != 'out_fc'\n",
    "    ]\n",
    "    if type(finetune_body_factor) is float:\n",
    "        print(\n",
    "            \"Setting up optimizer to fine-tune body with LR %.8f and head with LR %.5f\" %\n",
    "            (base_lr * finetune_body_factor, base_lr)\n",
    "        )\n",
    "        return [\n",
    "            {'params': body_parameters, 'lr': base_lr * finetune_body_factor},\n",
    "            {'params': model.out_fc.parameters(), 'lr': base_lr}\n",
    "        ]\n",
    "    else:\n",
    "        lower_bound_factor, upper_bound_factor = finetune_body_factor\n",
    "        print(\n",
    "            \"Setting up optimizer to fine-tune body with LR in range [%.8f, %.8f]\"\n",
    "            \" and head with LR %.5f\" %\n",
    "            (base_lr * lower_bound_factor, base_lr * upper_bound_factor, base_lr)\n",
    "        )\n",
    "        lrs = np.geomspace(\n",
    "            base_lr * lower_bound_factor, base_lr * upper_bound_factor,\n",
    "            len(body_parameters)\n",
    "        )\n",
    "        param_lr_maps = [\n",
    "            {'params': param, 'lr': lr} for ((_, param), lr) in\n",
    "            zip(body_parameters, lrs)\n",
    "        ]\n",
    "        param_lr_maps.append({'params': model.out_fc.parameters(), 'lr': base_lr})\n",
    "        return param_lr_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is useful for setting up parameter to LR mappings for fine-tuning the model. Specifically:\n",
    "* `model` should be a `torch.nn.Module`\n",
    "* `base_lr` should be a float, defining the LR for the linear head\n",
    "* `finetune_body_factor` should be a list of two floats: a lower bound factor and upper bound factor. The learning rate for the body of the model will be equally (log) spaced between (`base_lr` * `lower_bound_factor`) and (`base_lr` * `upper_bound_factor`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_optimizer_and_scheduler(param_lr_maps, base_lr, epochs, steps_per_epoch):\n",
    "    \"\"\"Create a PyTorch AdamW optimizer and OneCycleLR scheduler with `param_lr_maps` parameter mapping,\n",
    "       with base LR `base_lr`, for training for `epochs` epochs, with `steps_per_epoch` iterations\n",
    "       per epoch.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(param_lr_maps, lr=base_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, base_lr, epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def checkpoint_state(\n",
    "    model, epoch, optimizer, scheduler, train_loss, train_acc, val_loss, val_acc\n",
    "):\n",
    "    \"\"\"Checkpoint the state of the system, including `model` state, `optimizer` state, `scheduler`\n",
    "       state, for `epoch`, saving the metrics as well.\"\"\"\n",
    "    torch.save(\n",
    "        {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': None if scheduler is None else scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'epoch': epoch\n",
    "        },\n",
    "        os.path.join(model.save_dir, 'epoch_%d.pth' % epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, you can see how to set up the optimizer and scheduler to fine-tune using the one-cycle LR scheme. The linear head is fine-tuned with a learning rate of $10^{-3}$, and the body is fine-tuned with a learning rate spaced between $10^{-8}$ and $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "model = resnet18(pretrained=True, num_classes=2)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "mixup = True\n",
    "num_epochs = 5\n",
    "base_lr = 1e-3\n",
    "finetune_body_factor = [1e-5, 1e-2]\n",
    "param_lr_maps = get_param_lr_maps(model, base_lr, finetune_body_factor)\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(param_lr_maps, base_lr, num_epochs, len(tr_dl))\n",
    "criterion = {\n",
    "    'train': nn.CrossEntropyLoss(reduction='none' if mixup else 'mean'),\n",
    "    'val': nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple training loop would look like the following. Note that:\n",
    "* The one-cycle LR scheduler is passed in, and the logic for updating that is handled in `train`\n",
    "* Different criterion are used for training and validation. This is because the criterion for mixup is different for each batch, due to the mixing factor, so the criterion is modified in the loop for training, so the reduction is handled there, whereas reduction is standard when evaluating in validation (i.e. mean reduction)\n",
    "* Test-time augmentation is done in validation. Note that this will require having a special augmentation scheme, so validation transforms will need to be set appropriately. You can see above for an example of how to do that.\n",
    "* The model state is checkpointed each epoch. After checkpointing the state of the model and system, the directory where the state was saved can be accessed by inspecting `model.save_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /share/nikola/export/dt372/breakhis_gradcam/logs/2020-02-16-23-22-02/metrics.log\n",
      "Cleared all logging handlers\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "clear_logging_handlers = setup_logging_streams(model, log_to_file=True, log_to_stdout=False)\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss, tr_acc = train(\n",
    "        model, epoch + 1, tr_dl, criterion['train'], optimizer, scheduler=scheduler,\n",
    "        mixup=mixup, alpha=0.4, logging_frequency=25\n",
    "    )\n",
    "    val_loss, val_acc = validate(\n",
    "        model, epoch + 1, val_dl, criterion['val'], tta=True,\n",
    "        logging_frequency=25\n",
    "    )\n",
    "    checkpoint_state(\n",
    "        model, epoch + 1, optimizer, scheduler, tr_loss, tr_acc, val_loss, val_acc,\n",
    "    )\n",
    "clear_logging_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch_1.pth', 'epoch_2.pth', 'epoch_3.pth', 'epoch_4.pth', 'epoch_5.pth']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "os.listdir(model.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just use the validate method with some slight alterations to get the standard training accuracy (not the mixup accracy, which might not be as representative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy after 5 epochs is 0.96358\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "_, tr_acc_no_mixup = validate(model, epoch + 1, tr_dl, criterion['val'], tta=False, logging_frequency=25)\n",
    "print(\"Training accuracy after %d epochs is %.5f\" % (epoch + 1, tr_acc_no_mixup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /share/nikola/export/dt372/breakhis_gradcam/logs/2020-02-17-00-39-30/metrics.log\n",
      "New best validation loss found after epoch 1.\n",
      "New best validation loss found after epoch 3.\n",
      "New best validation loss found after epoch 11.\n",
      "New best validation loss found after epoch 25.\n",
      "New best validation loss found after epoch 33.\n",
      "New best validation loss found after epoch 34.\n",
      "New best validation loss found after epoch 35.\n",
      "New best validation loss found after epoch 36.\n",
      "New best validation loss found after epoch 41.\n",
      "New best validation loss found after epoch 45.\n",
      "Best model based on metrics was after epoch 45.\n",
      "Cleared all logging handlers\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\"\"\"My own experiment - running a full training session.\"\"\"\n",
    "from breakhis_gradcam.resnet import resnet34\n",
    "\n",
    "model = resnet34(pretrained=True, num_classes=2)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "mixup = True\n",
    "num_epochs = 50\n",
    "base_lr = 1e-3\n",
    "finetune_body_factor = [1e-5, 1e-2]\n",
    "param_lr_maps = get_param_lr_maps(model, base_lr, finetune_body_factor)\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(param_lr_maps, base_lr, num_epochs, len(tr_dl))\n",
    "criterion = {\n",
    "    'train': nn.CrossEntropyLoss(reduction='none' if mixup else 'mean'),\n",
    "    'val': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "clear_logging_handlers = setup_logging_streams(model, log_to_file=True, log_to_stdout=False)\n",
    "metrics = []\n",
    "best_val_epoch, best_val_loss, best_val_acc = -1, np.inf, 0.\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss, tr_acc = train(\n",
    "        model, epoch + 1, tr_dl, criterion['train'], optimizer, scheduler=scheduler,\n",
    "        mixup=mixup, alpha=0.4, logging_frequency=25\n",
    "    )\n",
    "    val_loss, val_acc = validate(\n",
    "        model, epoch + 1, val_dl, criterion['val'], tta=True,\n",
    "        logging_frequency=25\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"New best validation loss found after epoch %d.\" % (epoch + 1))\n",
    "        best_val_epoch, best_val_loss, best_val_acc = (\n",
    "            epoch + 1, val_loss, val_acc\n",
    "        )\n",
    "\n",
    "    checkpoint_state(\n",
    "        model, epoch + 1, optimizer, scheduler, tr_loss, tr_acc, val_loss, val_acc,\n",
    "    )\n",
    "    metrics.append((tr_loss, tr_acc, val_loss, val_acc))\n",
    "print(\"Best model based on metrics was after epoch %d.\" % (best_val_epoch))\n",
    "clear_logging_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1790834092730931 0.9723791588198368\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(best_val_loss, best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "best_model_state_dict = torch.load(os.path.join(model.save_dir, 'epoch_45.pth'))['model_state_dict']\n",
    "model.load_state_dict(best_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy after 50 epochs is 0.99367\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "_, tr_acc_no_mixup = validate(model, best_val_epoch + 1, tr_dl, criterion['val'], tta=False, logging_frequency=25)\n",
    "print(\"Training accuracy after %d epochs is %.5f\" % (epoch + 1, tr_acc_no_mixup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
