{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BreaKHis Data Processing\n",
    "\n",
    "> The BreaKHis dataset is organized by benign and malignant tumors, and then by the specific tumor types. The functions provided here will help to quickly (lazily) process the data for usage in training image classification models, while maintaining the additional information that might not be necessary. For example, if training on benign/malignant labels, the information about which specific tumor is present will still be available in the dataset definition. The data is anonymized, so there's no possibility of splitting at the patient level. Instead, we leave dataset splitting up to the user, but provide some utility functions to reproduce the results obtained in initial development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "np.random.seed(31)\n",
    "torch.manual_seed(31);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, the random seed (for both Numpy and PyTorch) are set to 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "data_dir = '/share/nikola/export/dt372/BreaKHis_v1/'\n",
    "label = 'tumor_type'\n",
    "_data_dir = os.path.join(data_dir, 'histology_slides/breast/')\n",
    "benign_dir = os.path.join(_data_dir, 'benign/SOB')\n",
    "benign_subtumors = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
    "benign_subdirs = [('benign', subtumor, os.path.join(benign_dir, subtumor)) for subtumor in benign_subtumors]\n",
    "malignant_dir = os.path.join(_data_dir, 'malignant/SOB')\n",
    "malignant_subtumors = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
    "malignant_subdirs = [('malignant', subtumor, os.path.join(malignant_dir, subtumor)) for subtumor in malignant_subtumors]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "benign_subsubdirs = flatten([\n",
    "    [(tumor, subtumor, subsubdir, os.path.join(subdir, subsubdir)) for subsubdir in os.listdir(subdir)]\n",
    "    for tumor, subtumor, subdir in benign_subdirs\n",
    "])\n",
    "malignant_subsubdirs = flatten([\n",
    "    [(tumor, subtumor, subsubdir, os.path.join(subdir, subsubdir)) for subsubdir in os.listdir(subdir)]\n",
    "    for tumor, subtumor, subdir in malignant_subdirs\n",
    "])\n",
    "for (_, _, _, benign_subsubdir), (_, _, _, malignant_subsubdir) in zip(benign_subsubdirs, malignant_subsubdirs):\n",
    "    assert os.path.isdir(benign_subsubdir), \"%s is not a valid directory\" % benign_subsubdir\n",
    "    assert os.path.isdir(malignant_subsubdir), \"%s is not a valid directory\" % malignant_subsubdir\n",
    "magnifications = ['40X', '100X', '200X', '400X']\n",
    "benign_data = []\n",
    "malignant_data = []\n",
    "for (tumor, subtumor, slide_number, benign_subsubdir) in benign_subsubdirs:\n",
    "    data_mapping = {}\n",
    "    slide_id = slide_number.split('_')[-1].split('-')[-1]\n",
    "    for magnification in magnifications:\n",
    "        magnification_subsubdir = os.path.join(benign_subsubdir, magnification)\n",
    "        for slide in os.listdir(magnification_subsubdir):\n",
    "            sequence_id = int(slide.split('.')[0].split('-')[-1])\n",
    "            if sequence_id not in data_mapping:\n",
    "                data_mapping[sequence_id] = {}\n",
    "            data_mapping[sequence_id][magnification] = (\n",
    "                os.path.join(magnification_subsubdir, slide),\n",
    "                tumor, subtumor, magnification, slide_id, sequence_id\n",
    "            )\n",
    "    benign_data.append((tumor, subtumor, slide_id, data_mapping))\n",
    "for (tumor, subtumor, slide_number, malignant_subsubdir) in malignant_subsubdirs:\n",
    "    data_mapping = {}\n",
    "    slide_id = slide_number.split('_')[-1].split('-')[-1]\n",
    "    for magnification in magnifications:\n",
    "        magnification_subsubdir = os.path.join(malignant_subsubdir, magnification)\n",
    "        for slide in os.listdir(magnification_subsubdir):\n",
    "            sequence_id = int(slide.split('.')[0].split('-')[-1])\n",
    "            if sequence_id not in data_mapping:\n",
    "                data_mapping[sequence_id] = {}\n",
    "            data_mapping[sequence_id][magnification] = (\n",
    "                os.path.join(magnification_subsubdir, slide),\n",
    "                tumor, subtumor, magnification, slide_id, sequence_id\n",
    "            )\n",
    "    malignant_data.append((tumor, subtumor, slide_id, data_mapping))\n",
    "\n",
    "all_data = []\n",
    "total = 0\n",
    "for tumor, subtumor, slide_id, data_mapping in benign_data:\n",
    "    for cell_id, file_mapping in data_mapping.items():\n",
    "        total += len(file_mapping)\n",
    "        all_data.append((\n",
    "            (tumor, subtumor, slide_id, file_mapping),\n",
    "            subtumor if label == 'tumor_type' else tumor\n",
    "        ))\n",
    "for tumor, subtumor, slide_id, data_mapping in malignant_data:\n",
    "    for cell_id, file_mapping in data_mapping.items():\n",
    "        total += len(file_mapping)\n",
    "        all_data.append((\n",
    "            (tumor, subtumor, slide_id, file_mapping),\n",
    "            subtumor if label == 'tumor_type' else tumor\n",
    "        ))\n",
    "assert total == 7909, \"Some images might be missing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BreaKHisDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" PyTorch dataset definition of the BreaKHis dataset.\n",
    "    \n",
    "    Construction of the dataset object should be done using this\n",
    "    class's method `initialize`. Simply providing the data directory\n",
    "    where the data was downloaded is sufficient.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_mapping = {\n",
    "        'tumor_class': {'benign': 0, 'malignant': 1},\n",
    "        'tumor_type': {\n",
    "            subtumor: i for i, subtumor in enumerate([\n",
    "                'adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma',\n",
    "                'ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma'   \n",
    "            ])\n",
    "        }\n",
    "    }\n",
    "    index_mapping = {\n",
    "        k: {iv: kv for (kv, iv) in v.items()} for (k, v) in label_mapping.items()\n",
    "    }\n",
    "\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (fp, tumor, subtumor, magnification, slide_id, sequence_id), label = self.dataset[index]\n",
    "        image = Image.open(fp)\n",
    "        data = self.transform(image) if image else image\n",
    "        return data, torch.Tensor([label]).long().squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def _split(\n",
    "        cls, dataset, split={'train': 0.8, 'val': 0.2}, criterion=['tumor_class'],\n",
    "        split_transforms={'train': None, 'val': None}\n",
    "    ):\n",
    "        \"\"\" Splits `dataset` according to `split` percentages and `criterion`\n",
    "        \n",
    "        Arguments:\n",
    "            `dataset`: the dataset to split (this is the output of `initialize`)\n",
    "            `split` (Dict[str, float]): a mapping of strings to floats corresponding to the percentage\n",
    "                in each split of the dataset; must add up to 1.\n",
    "            `criterion` (List[str]): one of 'tumor_class' (benign/malignant) or \n",
    "                                     'tumor_type' (e.g. adenosis) and/or 'magnification'\n",
    "            `split_transforms` (Dict[str, torchvision.transforms]): a mapping of split IDs to the\n",
    "                corresponding transforms.\n",
    "        Returns:\n",
    "            data_partitioned (Dict[str, List]): mapping of data corresponding to the split IDs\n",
    "                given in `split`, and each split ID maps to a `BreaKHisDataset` encompassing the\n",
    "                list of data points, with the proper percentage of the dataset allocated to each split ID.\n",
    "        \"\"\"\n",
    "        assert sum([v for (_, v) in split.items()]) == 1, 'Please specify proper split percentages to sum to 1.'\n",
    "        assert len(criterion) > 0, \"Must specify at least 1 criterion to split on\"\n",
    "        split_by_tumor_class = 'tumor_class' in criterion\n",
    "        split_by_tumor_type = 'tumor_type' in criterion\n",
    "        assert bool(split_by_tumor_class) != bool(split_by_tumor_type), \"Please only specify 1 of tumor type and class\"\n",
    "        split_by_magnification = 'magnification' in criterion\n",
    "        \n",
    "        data_split = {}\n",
    "        for (tumor_class, tumor_type, slide_id, slide_data_mapping), label in dataset:\n",
    "            if split_by_tumor_class or split_by_tumor_type:\n",
    "                split_key = tumor_class if split_by_tumor_class else tumor_type\n",
    "                if split_key not in data_split:\n",
    "                    data_split[split_key] = (\n",
    "                        {'40X': [], '100X': [], '200X': [], '400X': []}\n",
    "                        if split_by_magnification else []\n",
    "                    )\n",
    "                for magnification, fp in slide_data_mapping.items():\n",
    "                    queue = (\n",
    "                        data_split[split_key][magnification] if split_by_magnification\n",
    "                        else data_split[split_key]\n",
    "                    )\n",
    "                    queue.append((fp, label))\n",
    "            else:\n",
    "                for magnification, fp in slide_data_mapping.items():\n",
    "                    if magnification not in data_split:\n",
    "                        data_split[magnification] = []\n",
    "                    data_split[magnification].append((fp, label))\n",
    "\n",
    "        def partition(dataset):\n",
    "            permutation = np.random.permutation(range(len(dataset)))\n",
    "            init_percentage = 0.0\n",
    "            split_dataset = []\n",
    "            for split_id, split_percentage in split.items():\n",
    "                start_index = int(init_percentage * len(permutation))\n",
    "                end_index = int((init_percentage + split_percentage) * len(permutation))\n",
    "                split_dataset.append(\n",
    "                    (split_id, [dataset[index] for index in permutation[start_index:end_index]])\n",
    "                )\n",
    "                init_percentage += split_percentage\n",
    "            return split_dataset\n",
    "\n",
    "        for split_key, data_wrap in data_split.items():\n",
    "            if type(data_wrap) == dict:\n",
    "                for magnification, index_set in data_wrap.items():\n",
    "                    data_wrap[magnification] = partition(index_set)\n",
    "            else:\n",
    "                data_split[split_key] = partition(data_wrap)\n",
    "        \n",
    "        data_partitioned = {k: [] for (k, _) in split.items()}\n",
    "        for split_key, data_wrap in data_split.items():\n",
    "            if type(data_wrap) == dict:\n",
    "                for magnification, index_set in data_wrap.items():\n",
    "                    for split_id, data in index_set:\n",
    "                        data_partitioned[split_id].extend(data)\n",
    "            else:\n",
    "                for split_id, data in data_wrap:\n",
    "                    data_partitioned[split_id].extend(data)\n",
    "\n",
    "        return {k: cls(data_subset, split_transforms[k]) for k, data_subset in data_partitioned.items()}\n",
    "    \n",
    "    @classmethod\n",
    "    def initalize(\n",
    "        cls, data_dir, label='tumor_class', split={'train': 0.8, 'val': 0.2}, criterion=['tumor_class'],\n",
    "        split_transforms={'train': None, 'val': None}\n",
    "    ):\n",
    "        \"\"\"Initializes a PyTorch dataset object for the data contained in `data_dir`.\n",
    "           \n",
    "        Arguments:\n",
    "            `data_dir` (str): the directory where the BreaKHis dataset was downloaded\n",
    "            `label` (str): the label to use for the dataset (either 'tumor_class' or 'tumor_type')\n",
    "            `split` (Dict[str, float]): a mapping of strings to floats corresponding to the percentage\n",
    "                in each split of the dataset; must add up to 1.\n",
    "            `criterion` (List[str]): one of 'tumor_class' (benign/malignant) or \n",
    "                'tumor_type' (e.g. adenosis) and/or 'magnification'\n",
    "            `split_transforms` (Dict[str, torchvision.transforms]): a mapping of split IDs to the\n",
    "                corresponding transforms.\n",
    "        Returns:\n",
    "            data_partitioned (Dict[str, List]): mapping of data corresponding to the split IDs\n",
    "                given in `split`, and each split ID maps to a `BreaKHisDataset` encompassing the\n",
    "                list of data points, with the proper percentage of the dataset allocated to each split ID.\n",
    "        \"\"\"\n",
    "        assert label in ['tumor_class', 'tumor_type'], \"Please properly specify the label for this dataset.\"\n",
    "        for split_id in split:\n",
    "            assert split_id in split_transforms, \"\"\"Split ID '%s' is not included in split_transforms\"\"\" % split_id\n",
    "        _data_dir = os.path.join(data_dir, 'histology_slides/breast/')\n",
    "        benign_dir = os.path.join(_data_dir, 'benign/SOB')\n",
    "        benign_subtumors = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
    "        benign_subdirs = [('benign', subtumor, os.path.join(benign_dir, subtumor)) for subtumor in benign_subtumors]\n",
    "        malignant_dir = os.path.join(_data_dir, 'malignant/SOB')\n",
    "        malignant_subtumors = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
    "        malignant_subdirs = [('malignant', subtumor, os.path.join(malignant_dir, subtumor)) for subtumor in malignant_subtumors]\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        benign_subsubdirs = flatten([\n",
    "            [(tumor, subtumor, subsubdir, os.path.join(subdir, subsubdir)) for subsubdir in os.listdir(subdir)]\n",
    "            for tumor, subtumor, subdir in benign_subdirs\n",
    "        ])\n",
    "        malignant_subsubdirs = flatten([\n",
    "            [(tumor, subtumor, subsubdir, os.path.join(subdir, subsubdir)) for subsubdir in os.listdir(subdir)]\n",
    "            for tumor, subtumor, subdir in malignant_subdirs\n",
    "        ])\n",
    "        for (_, _, _, benign_subsubdir), (_, _, _, malignant_subsubdir) in zip(benign_subsubdirs, malignant_subsubdirs):\n",
    "            assert os.path.isdir(benign_subsubdir), \"%s is not a valid directory\" % benign_subsubdir\n",
    "            assert os.path.isdir(malignant_subsubdir), \"%s is not a valid directory\" % malignant_subsubdir\n",
    "        magnifications = ['40X', '100X', '200X', '400X']\n",
    "        benign_data = []\n",
    "        malignant_data = []\n",
    "        for (tumor, subtumor, slide_number, benign_subsubdir) in benign_subsubdirs:\n",
    "            data_mapping = {}\n",
    "            slide_id = slide_number.split('_')[-1].split('-')[-1]\n",
    "            for magnification in magnifications:\n",
    "                magnification_subsubdir = os.path.join(benign_subsubdir, magnification)\n",
    "                for slide in os.listdir(magnification_subsubdir):\n",
    "                    sequence_id = int(slide.split('.')[0].split('-')[-1])\n",
    "                    if sequence_id not in data_mapping:\n",
    "                        data_mapping[sequence_id] = {}\n",
    "                    data_mapping[sequence_id][magnification] = (\n",
    "                        os.path.join(magnification_subsubdir, slide),\n",
    "                        tumor, subtumor, magnification, slide_id, sequence_id\n",
    "                    )\n",
    "            benign_data.append((tumor, subtumor, slide_id, data_mapping))\n",
    "        for (tumor, subtumor, slide_number, malignant_subsubdir) in malignant_subsubdirs:\n",
    "            data_mapping = {}\n",
    "            slide_id = slide_number.split('_')[-1].split('-')[-1]\n",
    "            for magnification in magnifications:\n",
    "                magnification_subsubdir = os.path.join(malignant_subsubdir, magnification)\n",
    "                for slide in os.listdir(magnification_subsubdir):\n",
    "                    sequence_id = int(slide.split('.')[0].split('-')[-1])\n",
    "                    if sequence_id not in data_mapping:\n",
    "                        data_mapping[sequence_id] = {}\n",
    "                    data_mapping[sequence_id][magnification] = (\n",
    "                        os.path.join(magnification_subsubdir, slide),\n",
    "                        tumor, subtumor, magnification, slide_id, sequence_id\n",
    "                    )\n",
    "            malignant_data.append((tumor, subtumor, slide_id, data_mapping))\n",
    "            \n",
    "        all_data = []\n",
    "        total = 0\n",
    "        for tumor, subtumor, slide_id, data_mapping in benign_data:\n",
    "            for cell_id, file_mapping in data_mapping.items():\n",
    "                total += len(file_mapping)\n",
    "                all_data.append((\n",
    "                    (tumor, subtumor, slide_id, file_mapping),\n",
    "                    cls.label_mapping[label][subtumor if label == 'tumor_type' else tumor]\n",
    "                ))\n",
    "        for tumor, subtumor, slide_id, data_mapping in malignant_data:\n",
    "            for cell_id, file_mapping in data_mapping.items():\n",
    "                total += len(file_mapping)\n",
    "                all_data.append((\n",
    "                    (tumor, subtumor, slide_id, file_mapping),\n",
    "                    cls.label_mapping[label][subtumor if label == 'tumor_type' else tumor]\n",
    "                ))\n",
    "        assert total == 7909, \"Some images might be missing.\"\n",
    "        \n",
    "        return cls._split(\n",
    "            all_data, split=split, criterion=criterion, split_transforms=split_transforms\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def initialize_datasets(\n",
    "    data_dir, label='tumor_class', split={'train': 0.8, 'val': 0.2}, criterion=['tumor_class'],\n",
    "    split_transforms={'train': None, 'val': None}\n",
    "):\n",
    "    \"\"\"Returns a `BreaKHisDataset` object for the data contained in `data_dir`.\"\"\"\n",
    "    return BreaKHisDataset.initalize(\n",
    "        data_dir, label=label, criterion=criterion, split=split, split_transforms=split_transforms\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tumor_class': {0: 'benign', 1: 'malignant'},\n",
       " 'tumor_type': {0: 'adenosis',\n",
       "  1: 'fibroadenoma',\n",
       "  2: 'phyllodes_tumor',\n",
       "  3: 'tubular_adenoma',\n",
       "  4: 'ductal_carcinoma',\n",
       "  5: 'lobular_carcinoma',\n",
       "  6: 'mucinous_carcinoma',\n",
       "  7: 'papillary_carcinoma'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "BreaKHisDataset.index_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset, you only need one function calls. Within this function call:\n",
    "* You can specify the label type when initializing the dataset by specifying `label` in `initialize`\n",
    "    * It must be 1 of 'tumor_class' or 'tumor_type'\n",
    "* You can make arbitrary splits of the data (within reason) when splitting the dataset via `split_dataset`\n",
    "* You can make sure to split equally within various criterion using `criterion`, which can include tumor class/tumor type, and magnification.\n",
    "    * You can not split equally by both tumor class and tumor type (error will be thrown if attempted).\n",
    "* You can use different transforms for different splits using `split_transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.RandomHorizontalFlip(0.8),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                        (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mapping = initialize_datasets(\n",
    "    '/share/nikola/export/dt372/BreaKHis_v1/',\n",
    "    label='tumor_type', criterion=['tumor_type', 'magnification'],\n",
    "    split_transforms={'train': train_transform, 'val': val_transform}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds, val_ds = ds_mapping['train'], ds_mapping['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.4291, -2.4291, -2.4291,  ...,  0.8082,  0.7888,  0.8082],\n",
       "          [-2.4291, -2.4291, -2.4291,  ...,  0.8082,  0.7888,  0.7888],\n",
       "          [-2.4291, -2.4291, -2.4291,  ...,  0.8082,  0.8082,  0.7501],\n",
       "          ...,\n",
       "          [ 0.6725,  0.6144,  0.2654,  ...,  0.4399,  0.3624,  0.0522],\n",
       "          [ 0.4981,  0.6338,  0.5174,  ...,  0.3236,  0.3817,  0.1297],\n",
       "          [ 0.3430,  0.3624,  0.4593,  ...,  0.3624,  0.3042,  0.0522]],\n",
       " \n",
       "         [[-2.4183, -2.4183, -2.4183,  ...,  0.9251,  0.9251,  0.9251],\n",
       "          [-2.4183, -2.4183, -2.4183,  ...,  0.9251,  0.9054,  0.9251],\n",
       "          [-2.4183, -2.4183, -2.4183,  ...,  0.9251,  0.9251,  0.9054],\n",
       "          ...,\n",
       "          [ 0.2564,  0.2564, -0.0189,  ...,  0.5121,  0.4531,  0.0598],\n",
       "          [ 0.0598,  0.1778,  0.1581,  ...,  0.3744,  0.4138,  0.1188],\n",
       "          [-0.1566, -0.0976,  0.0204,  ...,  0.4531,  0.3548,  0.0991]],\n",
       " \n",
       "         [[-2.2214, -2.2214, -2.2214,  ...,  0.8027,  0.8027,  0.8027],\n",
       "          [-2.2214, -2.2214, -2.2214,  ...,  0.7832,  0.7637,  0.7832],\n",
       "          [-2.2214, -2.2214, -2.2214,  ...,  0.8027,  0.7832,  0.7832],\n",
       "          ...,\n",
       "          [ 1.0368,  0.9978,  0.7052,  ...,  0.9588,  0.9588,  0.6661],\n",
       "          [ 0.8807,  0.9978,  0.9198,  ...,  0.8807,  0.9783,  0.8027],\n",
       "          [ 0.7442,  0.8027,  0.8612,  ...,  0.9588,  0.9588,  0.7637]]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, it is very simple to create the dataloaders for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(tr_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "All images in this dataset are captured from an ROI determined by a professional pathologist, so all images are assumed to have *a* tumor.\n",
    "\n",
    "### Samples\n",
    "\n",
    "* Samples are generated from breast tissue biopsy slides, stained with hematoxylin and eosin (HE).\n",
    "* Prepared for histological study and labelled by pathologists of the P&D Lab\n",
    "* Breast tumor specimens assessed by Immunohistochemistry (IHC)\n",
    "* Core Needle Biopsy (CNB) and Surgical Open Biopsy (SOB)\n",
    "* Section of ~3µm thickness\n",
    "\n",
    "### Image acquisition\n",
    "* Olympus BX-50 system microscope with a relay lens with magnification of 3.3× coupled to a Samsung digital color camera SCC-131AN\n",
    "* Magnification 40×, 100×, 200×, and 400× (objective lens 4×, 10×, 20×, and 40× with ocular lens 10×)\n",
    "* Camera pixel size 6.5 µm\n",
    "* Raw images without normalization nor color color standardization\n",
    "* Resulting images saved in 3-channel RGB, 8-bit depth in each channel, PNG format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
